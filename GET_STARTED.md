# What to Do Next - Getting Started Guide

## ‚ö° Start Here (5 minutes)

### Step 1: Open Terminal
```bash
# Open PowerShell in C:\Users\legot\Chrona
cd C:\Users\legot\Chrona\rust
```

### Step 2: Build and Run
```bash
cargo run -p ui --release
```

**Wait time**: ~30 seconds on first run (compiling optimizations)

### Step 3: Test LLM Integration
Once the UI window opens:

1. **See three tier panels** (Mini, Regular, Pro side-by-side)
2. **Click "Ask AI" button** on the **left (Mini) panel**
3. **Type a health question**:
   - "Is my heart rate of 75 bpm healthy?"
   - "What does elevated heart rate mean?"
   - "Any tips to reduce stress?"
4. **Click "Ask LLM" button**
5. **Wait 5-20 seconds** (model thinking...)
6. **See response** in modal dialog

---

## üìö Read Next (Choose Your Path)

### Path A: Just Want to Test It ‚ú®
‚Üí See [QUICK_START.md](QUICK_START.md) (5 minutes)
- How to run the app
- What buttons do what
- Expected behavior

### Path B: Want Technical Details üîß
‚Üí See [llm-bindings-guide.md](docs/llm-bindings-guide.md) (15 minutes)
- How the LLM loading works
- Subprocess execution details
- Performance expectations
- Tier-specific constraints

### Path C: Want Full Context üìã
‚Üí See [LLM_INTEGRATION_STATUS.md](docs/LLM_INTEGRATION_STATUS.md) (20 minutes)
- Complete status report
- All implemented features
- Troubleshooting guide
- What's next in development

### Path D: Want Executive Summary üëî
‚Üí See [README_LLM_INTEGRATION.md](README_LLM_INTEGRATION.md) (10 minutes)
- What's implemented
- How it works
- Key decisions made

---

## ‚úÖ Verify Everything is Working

### Test 1: Model File
```powershell
# Check the model exists and is the right size
Get-Item C:\Users\legot\Chrona\data\mini\model-3b.llamafile | Select-Object Length

# Should show: ~2,886,750,498 bytes (2.69 GB)
```

### Test 2: Direct Model Execution
```powershell
# Run the model directly (no UI)
& "C:\Users\legot\Chrona\data\mini\model-3b.llamafile" `
  -c 256 -n 32 -p "Is my heart rate healthy?" -t 4

# Should generate a response (may take 5-20 seconds)
```

### Test 3: Full App
```bash
cd C:\Users\legot\Chrona\rust
cargo run -p ui --release

# Click "Ask AI" and try a prompt
```

---

## üêõ If Something Goes Wrong

### Issue: App won't start
```bash
# Check for build errors
cd C:\Users\legot\Chrona\rust
cargo build
```

### Issue: "Ask AI" shows error
```powershell
# Verify model file exists
Test-Path "C:\Users\legot\Chrona\data\mini\model-3b.llamafile"

# Check size
(Get-Item "C:\Users\legot\Chrona\data\mini\model-3b.llamafile").Length / 1GB
```

### Issue: Inference hangs
- Reduce token limit in [ui/src/tier_engine.rs](rust/ui/src/tier_engine.rs#L48):
  ```rust
  Tier::Mini8 => (256, 32),  // Change 64 to 32
  ```
- Rebuild: `cargo build -p ui`

### Issue: Out of memory
- Download Q4_K_M model (~1.9 GB instead of Q6_K)
- More info: See [model-quantization.md](docs/model-quantization.md)

---

## üéØ What You Should Expect

### ‚úÖ Should Work
- App window opens with three tier panels
- Heart rate values update (changing by ~¬±5 bpm)
- "Ask AI" button opens dialog
- Model loads and runs
- Response appears after 5-20 seconds
- Dialog closes cleanly

### ‚ö†Ô∏è May Be Slow
- First inference takes 10-20 seconds (depends on CPU)
- Subsequent inferences similar speed
- Responses may be brief or simple (depends on model)

### ‚ùå Not Yet Implemented
- GPU acceleration (CPU-only for now)
- Response streaming (full response at once)
- Storage of responses (Mini MVP feature)
- Real sensor data (using synthetic for now)

---

## üìç Key Locations

### Source Code
- **LLM implementation**: [rust/llm-runtime/src/lib.rs](rust/llm-runtime/src/lib.rs)
- **UI integration**: [rust/ui/src/tier_engine.rs](rust/ui/src/tier_engine.rs)
- **Button handler**: [rust/ui/src/main.rs](rust/ui/src/main.rs#L100-L115)

### Model File
- **Location**: [data/mini/model-3b.llamafile](data/mini/model-3b.llamafile)
- **Size**: 2.69 GB
- **Type**: Llama 3.2 3B Instruct (Q6_K quantization)

### Documentation
- **Quick start**: [QUICK_START.md](QUICK_START.md)
- **Technical guide**: [docs/llm-bindings-guide.md](docs/llm-bindings-guide.md)
- **Full status**: [docs/LLM_INTEGRATION_STATUS.md](docs/LLM_INTEGRATION_STATUS.md)
- **This session**: [SESSION_SUMMARY.md](SESSION_SUMMARY.md)

---

## üöÄ What's Planned Next

### This Sprint (Complete) ‚úÖ
- [x] LLM model loading
- [x] UI "Ask AI" button
- [x] Subprocess inference
- [x] Documentation

### Next Sprint üîÑ
- [ ] SQLite storage backend
- [ ] Heart rate anomaly detector
- [ ] Watch screens (Home/Detail/Alert)
- [ ] Real sensor data pipeline

### Future üìã
- [ ] GPU acceleration
- [ ] Response streaming
- [ ] Regular/Pro tier models
- [ ] Voice input/output

---

## üí° Tips & Tricks

### Run in Debug Mode
```bash
RUST_LOG=debug cargo run -p ui
```

### Run Release Binary Directly
```bash
# Build release
cargo build -p ui --release

# Run it
.\target\release\chrona.exe
```

### Change Inference Parameters
Edit [ui/src/tier_engine.rs](rust/ui/src/tier_engine.rs), then rebuild:
```rust
// Mini tier: 256 context, 64 tokens
// Change to:
// (256, 32) for faster inference
// (512, 128) for more detailed responses
```

### Test Different Prompts
Try asking the model:
- Health-related: "Is my heart rate normal?"
- Advice: "What should I do about stress?"
- Explanations: "What does high blood pressure mean?"
- Data interpretation: "My heart rate went from 60 to 100, what does that mean?"

---

## üìû Need Help?

### Quick Questions
See [QUICK_START.md](QUICK_START.md) or this file

### Technical Details
See [docs/llm-bindings-guide.md](docs/llm-bindings-guide.md)

### Troubleshooting
See [docs/LLM_INTEGRATION_STATUS.md#troubleshooting](docs/LLM_INTEGRATION_STATUS.md#troubleshooting)

### Everything
See [SESSION_SUMMARY.md](SESSION_SUMMARY.md)

---

## ‚ú® Summary

You now have a working LLM inference system integrated into Chrona:

1. **Model ready**: Llama 3.2 3B in place
2. **Code ready**: RealLlm struct implemented
3. **UI ready**: "Ask AI" button functional
4. **Docs ready**: 6 comprehensive guides

**Next step**: Run `cargo run -p ui --release` and click "Ask AI"!

---

**Status**: ‚úÖ READY TO USE  
**Build**: ‚úÖ PASSING  
**Model**: ‚úÖ VERIFIED  
**Docs**: ‚úÖ COMPLETE
